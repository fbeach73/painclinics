{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEO Audit — PainClinics.com\n",
    "\n",
    "Analyzes 16 months of Google Search Console data across 6 dimensions:\n",
    "1. Query Analysis & Clustering\n",
    "2. Page Performance\n",
    "3. Indexing & Coverage\n",
    "4. 404 Recovery\n",
    "5. \"Near Me\" & Generic Query Deep Dive\n",
    "6. Action Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "AUDIT_DIR = Path('.')\n",
    "if not (AUDIT_DIR / 'painclinics.com-Performance-on-Search-2026-02-25 - Queries.csv').exists():\n",
    "    AUDIT_DIR = Path('docs/seo-audit')\n",
    "\n",
    "print(f'Audit directory: {AUDIT_DIR.resolve()}')\n",
    "print('Files:', [f.name for f in AUDIT_DIR.iterdir() if f.suffix in ('.csv', '.json')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load CSVs\nqueries_file = list(AUDIT_DIR.glob('*Queries*.csv'))[0]\npages_file = list(AUDIT_DIR.glob('*Pages*.csv'))[0]\ncoverage_file = list(AUDIT_DIR.glob('*Coverage*.csv'))[0]\n\nqueries = pd.read_csv(queries_file, dtype_backend='numpy_nullable')\npages = pd.read_csv(pages_file, dtype_backend='numpy_nullable')\ncoverage = pd.read_csv(coverage_file, dtype_backend='numpy_nullable')\n\n# Normalize CTR from percentage string to float\nfor df in [queries, pages]:\n    df['CTR'] = df['CTR'].astype(str).str.rstrip('%').astype(float)\n    df['Clicks'] = pd.to_numeric(df['Clicks'], errors='coerce')\n    df['Impressions'] = pd.to_numeric(df['Impressions'], errors='coerce')\n    df['Position'] = pd.to_numeric(df['Position'], errors='coerce')\n\n# Rename first column for consistency\nqueries.rename(columns={queries.columns[0]: 'Query'}, inplace=True)\npages.rename(columns={pages.columns[0]: 'Page'}, inplace=True)\n\nprint(f'Queries: {len(queries)} rows')\nprint(f'Pages: {len(pages)} rows')\nprint(f'Coverage: {len(coverage)} rows')\nprint(f'Queries dtypes: {dict(queries.dtypes)}')\nprint()\nqueries.head(3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1A: Query Analysis & Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Condition and treatment keyword lists ---\nCONDITIONS = [\n    'back pain', 'neck pain', 'sciatica', 'neuropathy', 'fibromyalgia',\n    'arthritis', 'migraine', 'headache', 'knee pain', 'hip pain',\n    'shoulder pain', 'chronic pain', 'joint pain', 'nerve pain',\n    'herniated disc', 'spinal stenosis', 'degenerative disc',\n    'carpal tunnel', 'plantar fasciitis', 'complex regional',\n    'crps', 'radiculopathy', 'myofascial', 'tendonitis',\n    'whiplash', 'post surgical pain', 'cancer pain',\n    'pelvic pain', 'abdominal pain', 'chest pain',\n]\n\nTREATMENTS = [\n    'injection', 'nerve block', 'epidural', 'steroid',\n    'physical therapy', 'spinal cord stimulator', 'spinal cord stimulation',\n    'radiofrequency ablation', 'rfa', 'acupuncture', 'chiropractic',\n    'massage therapy', 'regenerative', 'prp', 'platelet rich plasma',\n    'stem cell', 'ketamine', 'infusion', 'tens', 'biofeedback',\n    'cognitive behavioral', 'medication management', 'opioid',\n    'suboxone', 'trigger point', 'facet joint', 'si joint',\n    'kyphoplasty', 'vertebroplasty', 'discography', 'intrathecal pump',\n    'botox', 'cortisone',\n]\n\n# US state names and abbreviations for city-state detection\nSTATES = [\n    'alabama','alaska','arizona','arkansas','california','colorado',\n    'connecticut','delaware','florida','georgia','hawaii','idaho',\n    'illinois','indiana','iowa','kansas','kentucky','louisiana',\n    'maine','maryland','massachusetts','michigan','minnesota',\n    'mississippi','missouri','montana','nebraska','nevada',\n    'new hampshire','new jersey','new mexico','new york',\n    'north carolina','north dakota','ohio','oklahoma','oregon',\n    'pennsylvania','rhode island','south carolina','south dakota',\n    'tennessee','texas','utah','vermont','virginia','washington',\n    'west virginia','wisconsin','wyoming',\n    ' al ', ' ak ', ' az ', ' ar ', ' ca ', ' co ', ' ct ', ' de ',\n    ' fl ', ' ga ', ' hi ', ' id ', ' il ', ' in ', ' ia ', ' ks ',\n    ' ky ', ' la ', ' me ', ' md ', ' ma ', ' mi ', ' mn ', ' ms ',\n    ' mo ', ' mt ', ' ne ', ' nv ', ' nh ', ' nj ', ' nm ', ' ny ',\n    ' nc ', ' nd ', ' oh ', ' ok ', ' or ', ' pa ', ' ri ', ' sc ',\n    ' sd ', ' tn ', ' tx ', ' ut ', ' vt ', ' va ', ' wa ', ' wv ',\n    ' wi ', ' wy ',\n]\n\ndef classify_query(q):\n    q_lower = f' {q.lower()} '\n    \n    if 'painclinics' in q_lower or 'pain clinics directory' in q_lower:\n        return 'brand'\n    \n    if any(kw in q_lower for kw in ['near me', 'near ', 'nearby', 'close to me']):\n        return 'near-me'\n    \n    if any(kw in q_lower for kw in CONDITIONS):\n        return 'condition'\n    \n    if any(kw in q_lower for kw in TREATMENTS):\n        return 'treatment'\n    \n    if any(st in q_lower for st in STATES):\n        return 'city-state'\n    \n    # Check for common clinic-name patterns: \"dr \", \"clinic\", \"center\", \"associates\", etc.\n    clinic_indicators = ['dr ', 'dr.', 'clinic', 'center', 'associates', 'medical', 'health',\n                         'wellness', 'rehab', 'institute', 'group', 'specialists',\n                         'physicians', 'care ', 'pllc', 'llc', 'md ']\n    if any(kw in q_lower for kw in clinic_indicators):\n        return 'clinic-name'\n    \n    if 'pain management' in q_lower or 'pain doctor' in q_lower or 'pain specialist' in q_lower:\n        return 'generic'\n    \n    return 'generic'\n\n# Ensure numeric types for aggregation (pandas 3.0 uses StringDtype by default)\nfor col in ['Clicks', 'Impressions', 'Position', 'CTR']:\n    queries[col] = pd.to_numeric(queries[col], errors='coerce')\n\nqueries['Cluster'] = queries['Query'].apply(classify_query)\n\nprint('Query cluster distribution:')\ncluster_summary = queries.groupby('Cluster').agg(\n    count=('Query', 'size'),\n    total_impressions=('Impressions', 'sum'),\n    total_clicks=('Clicks', 'sum'),\n    avg_position=('Position', 'mean'),\n    avg_ctr=('CTR', 'mean'),\n).sort_values('total_impressions', ascending=False)\ncluster_summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top queries by impressions (where Google thinks you're relevant)\n",
    "print('Top 25 queries by impressions:')\n",
    "queries.nlargest(25, 'Impressions')[['Query', 'Clicks', 'Impressions', 'CTR', 'Position', 'Cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTR by position bucket\n",
    "def position_bucket(pos):\n",
    "    if pos <= 3:\n",
    "        return '1-3'\n",
    "    elif pos <= 10:\n",
    "        return '4-10'\n",
    "    elif pos <= 20:\n",
    "        return '11-20'\n",
    "    else:\n",
    "        return '21+'\n",
    "\n",
    "queries['PosBucket'] = queries['Position'].apply(position_bucket)\n",
    "\n",
    "bucket_stats = queries.groupby('PosBucket').agg(\n",
    "    queries_count=('Query', 'size'),\n",
    "    total_impressions=('Impressions', 'sum'),\n",
    "    total_clicks=('Clicks', 'sum'),\n",
    "    avg_ctr=('CTR', 'mean'),\n",
    ").reindex(['1-3', '4-10', '11-20', '21+'])\n",
    "bucket_stats['weighted_ctr'] = (bucket_stats['total_clicks'] / bucket_stats['total_impressions'] * 100).round(2)\n",
    "\n",
    "print('CTR by position bucket:')\n",
    "bucket_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick wins: position 4-15, 50+ impressions — sorted by impressions desc\n",
    "quick_wins = queries[\n",
    "    (queries['Position'] >= 4) & \n",
    "    (queries['Position'] <= 15) & \n",
    "    (queries['Impressions'] >= 50)\n",
    "].sort_values('Impressions', ascending=False).copy()\n",
    "\n",
    "print(f'Quick wins: {len(quick_wins)} queries at position 4-15 with 50+ impressions')\n",
    "quick_wins.head(20)[['Query', 'Clicks', 'Impressions', 'CTR', 'Position', 'Cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gap analysis: high impressions but position >10 (ranking opportunity)\n",
    "gaps = queries[\n",
    "    (queries['Position'] > 10) & \n",
    "    (queries['Impressions'] >= 100)\n",
    "].sort_values('Impressions', ascending=False)\n",
    "\n",
    "print(f'Ranking opportunities: {len(gaps)} queries with 100+ impressions but position >10')\n",
    "gaps.head(20)[['Query', 'Clicks', 'Impressions', 'CTR', 'Position', 'Cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTR anomalies: position <5 but CTR <3% — title/description problems\n",
    "ctr_anomalies = queries[\n",
    "    (queries['Position'] < 5) & \n",
    "    (queries['CTR'] < 3) &\n",
    "    (queries['Impressions'] >= 50)\n",
    "].sort_values('Impressions', ascending=False)\n",
    "\n",
    "print(f'CTR anomalies: {len(ctr_anomalies)} queries with position <5 but CTR <3%')\n",
    "ctr_anomalies.head(20)[['Query', 'Clicks', 'Impressions', 'CTR', 'Position', 'Cluster']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1B: Page Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Classify page types from URL\ndef classify_page(url):\n    path = urlparse(url).path.rstrip('/')\n    query = urlparse(url).query\n    \n    if path == '' or path == '/':\n        if 'clinics=' in query:\n            return 'clinic-old-url'  # Legacy /?clinics= format\n        return 'homepage'\n    \n    if path.startswith('/pain-management/'):\n        parts = path.replace('/pain-management/', '').strip('/').split('/')\n        if len(parts) == 1:\n            slug = parts[0]\n            # State pages are 2-letter abbreviations\n            if re.match(r'^[a-z]{2}$', slug):\n                return 'state'\n            return 'clinic'\n        elif len(parts) == 2:\n            # state/city\n            if re.match(r'^[a-z]{2}$', parts[0]):\n                return 'city'\n            return 'clinic'\n        return 'clinic'\n    \n    if path.startswith('/blog'):\n        return 'blog'\n    \n    if path in ['/about', '/faq', '/contact', '/privacy', '/terms', '/claim']:\n        return 'static'\n    \n    if path.startswith('/directory') or path.startswith('/search'):\n        return 'directory'\n    \n    return 'other'\n\n# Ensure numeric types for pages too\nfor col in ['Clicks', 'Impressions', 'Position', 'CTR']:\n    pages[col] = pd.to_numeric(pages[col], errors='coerce')\n\npages['PageType'] = pages['Page'].apply(classify_page)\n\nprint('Page type distribution:')\ntype_summary = pages.groupby('PageType').agg(\n    count=('Page', 'size'),\n    total_impressions=('Impressions', 'sum'),\n    total_clicks=('Clicks', 'sum'),\n    avg_position=('Position', 'mean'),\n    avg_ctr=('CTR', 'mean'),\n).sort_values('total_impressions', ascending=False)\ntype_summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top pages by impressions (not clicks) — shows underperforming pages\n",
    "print('Top 25 pages by impressions:')\n",
    "pages.nlargest(25, 'Impressions')[['Page', 'Clicks', 'Impressions', 'CTR', 'Position', 'PageType']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pages with high impressions but low CTR — meta title/description issues\n",
    "underperforming_pages = pages[\n",
    "    (pages['Impressions'] >= 500) &\n",
    "    (pages['CTR'] < 2)\n",
    "].sort_values('Impressions', ascending=False)\n",
    "\n",
    "print(f'Underperforming pages: {len(underperforming_pages)} pages with 500+ impressions and CTR <2%')\n",
    "underperforming_pages.head(15)[['Page', 'Clicks', 'Impressions', 'CTR', 'Position', 'PageType']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orphaned pages: zero clicks despite some impressions\n",
    "orphaned = pages[\n",
    "    (pages['Clicks'] == 0) &\n",
    "    (pages['Impressions'] >= 10)\n",
    "].sort_values('Impressions', ascending=False)\n",
    "\n",
    "print(f'Orphaned pages (0 clicks, 10+ impressions): {len(orphaned)}')\n",
    "orphaned.head(15)[['Page', 'Impressions', 'Position', 'PageType']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1C: Indexing & Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The coverage CSV from GSC has date-level data. Analyze the most recent snapshot.\nprint('Coverage data columns:', coverage.columns.tolist())\nprint()\n\n# Parse the date-based coverage data\ncoverage['Date'] = pd.to_datetime(coverage['Date'])\n\n# Convert numeric columns\nfor col in ['Indexed', 'Not indexed', 'Impressions']:\n    if col in coverage.columns:\n        coverage[col] = pd.to_numeric(coverage[col], errors='coerce')\n\n# Get the most recent row with both indexed and not-indexed values\nindexed_col = 'Indexed' if 'Indexed' in coverage.columns else None\nif indexed_col:\n    recent = coverage.dropna(subset=[indexed_col]).tail(5)\n    print('Recent coverage snapshots:')\n    display(recent)\nelse:\n    print('No \"Indexed\" column found in coverage data')\n    print(coverage.head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Coverage trend\nnot_indexed_col = 'Not indexed'\nindexed_col = 'Indexed'\n\nif indexed_col in coverage.columns and not_indexed_col in coverage.columns:\n    coverage_with_data = coverage.dropna(subset=[indexed_col, not_indexed_col])\n    if len(coverage_with_data) > 0:\n        latest = coverage_with_data.iloc[-1]\n        indexed_val = float(latest[indexed_col])\n        not_indexed_val = float(latest[not_indexed_col])\n        print(f\"Latest coverage snapshot ({latest['Date'].strftime('%Y-%m-%d')}):\")\n        print(f\"  Indexed: {indexed_val:.0f}\")\n        print(f\"  Not indexed: {not_indexed_val:.0f}\")\n        total = indexed_val + not_indexed_val\n        if total > 0:\n            pct = indexed_val / total * 100\n            print(f\"  Index rate: {pct:.1f}%\")\n    else:\n        print('No rows with both Indexed and Not indexed values.')\nelse:\n    print('Coverage CSV does not have Indexed/Not indexed breakdown.')\n    print('Available columns:', coverage.columns.tolist())\n    print()\n    print('Raw coverage data sample:')\n    print(coverage.head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1D: 404 Recovery Audit\n",
    "\n",
    "Cross-reference any known 404 patterns with pages that still receive impressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for legacy /?clinics= URLs still getting impressions\n",
    "legacy_urls = pages[pages['PageType'] == 'clinic-old-url'].sort_values('Impressions', ascending=False)\n",
    "\n",
    "print(f'Legacy /?clinics= URLs still getting impressions: {len(legacy_urls)}')\n",
    "print(f'Total impressions on legacy URLs: {legacy_urls[\"Impressions\"].sum():,}')\n",
    "print(f'Total clicks on legacy URLs: {legacy_urls[\"Clicks\"].sum():,}')\n",
    "print()\n",
    "legacy_urls.head(15)[['Page', 'Clicks', 'Impressions', 'CTR', 'Position']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pages with www vs non-www — potential duplicate/redirect issues\n",
    "www_pages = pages[pages['Page'].str.contains('://www.', regex=False)]\n",
    "\n",
    "print(f'Pages with www prefix: {len(www_pages)}')\n",
    "if len(www_pages) > 0:\n",
    "    print(f'Total impressions on www pages: {www_pages[\"Impressions\"].sum():,}')\n",
    "    www_pages.head(10)[['Page', 'Clicks', 'Impressions', 'CTR', 'Position']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1E: \"Near Me\" & Generic Query Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All \"near me\" queries\n",
    "near_me = queries[queries['Cluster'] == 'near-me'].sort_values('Impressions', ascending=False)\n",
    "\n",
    "print(f'\"Near me\" queries: {len(near_me)}')\n",
    "print(f'Total \"near me\" impressions: {near_me[\"Impressions\"].sum():,}')\n",
    "print(f'Total \"near me\" clicks: {near_me[\"Clicks\"].sum():,}')\n",
    "print(f'Average position: {near_me[\"Position\"].mean():.1f}')\n",
    "print()\n",
    "near_me.head(20)[['Query', 'Clicks', 'Impressions', 'CTR', 'Position']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition-specific queries — what conditions drive the most impressions?\n",
    "condition_queries = queries[queries['Cluster'] == 'condition'].sort_values('Impressions', ascending=False)\n",
    "\n",
    "print(f'Condition queries: {len(condition_queries)}')\n",
    "print(f'Total impressions: {condition_queries[\"Impressions\"].sum():,}')\n",
    "print()\n",
    "condition_queries.head(20)[['Query', 'Clicks', 'Impressions', 'CTR', 'Position']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treatment-specific queries\n",
    "treatment_queries = queries[queries['Cluster'] == 'treatment'].sort_values('Impressions', ascending=False)\n",
    "\n",
    "print(f'Treatment queries: {len(treatment_queries)}')\n",
    "print(f'Total impressions: {treatment_queries[\"Impressions\"].sum():,}')\n",
    "print()\n",
    "treatment_queries.head(20)[['Query', 'Clicks', 'Impressions', 'CTR', 'Position']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic queries (pain management, pain doctor, etc.) — highest-value non-branded\n",
    "generic_queries = queries[queries['Cluster'] == 'generic'].sort_values('Impressions', ascending=False)\n",
    "\n",
    "print(f'Generic queries: {len(generic_queries)}')\n",
    "print(f'Total impressions: {generic_queries[\"Impressions\"].sum():,}')\n",
    "print()\n",
    "generic_queries.head(20)[['Query', 'Clicks', 'Impressions', 'CTR', 'Position']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content gap analysis: treatment/condition queries with no dedicated page\n",
    "# These queries have high impressions but the site likely ranks with homepage or clinic pages\n",
    "high_value_nonbrand = queries[\n",
    "    queries['Cluster'].isin(['near-me', 'condition', 'treatment', 'generic', 'city-state']) &\n",
    "    (queries['Impressions'] >= 50)\n",
    "].sort_values('Impressions', ascending=False)\n",
    "\n",
    "print(f'High-value non-brand queries (50+ impressions): {len(high_value_nonbrand)}')\n",
    "print(f'Total impressions: {high_value_nonbrand[\"Impressions\"].sum():,}')\n",
    "print()\n",
    "\n",
    "# Group by cluster\n",
    "hv_by_cluster = high_value_nonbrand.groupby('Cluster').agg(\n",
    "    queries=('Query', 'size'),\n",
    "    total_impressions=('Impressions', 'sum'),\n",
    "    avg_position=('Position', 'mean'),\n",
    ").sort_values('total_impressions', ascending=False)\n",
    "hv_by_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1F: Action Items Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_items = []\n",
    "\n",
    "# 1. Meta title/description rewrites for CTR anomalies\n",
    "for _, row in ctr_anomalies.iterrows():\n",
    "    action_items.append({\n",
    "        'priority': 'high' if row['Impressions'] >= 200 else 'medium',\n",
    "        'type': 'meta',\n",
    "        'query_or_url': row['Query'],\n",
    "        'action': f'Rewrite meta title/description — position {row[\"Position\"]:.1f} but only {row[\"CTR\"]:.1f}% CTR',\n",
    "        'impressions': row['Impressions'],\n",
    "        'expected_impact': 'Improve CTR from current position'\n",
    "    })\n",
    "\n",
    "# 2. Quick win content optimization\n",
    "for _, row in quick_wins.head(30).iterrows():\n",
    "    if row['Cluster'] in ('near-me', 'condition', 'treatment', 'generic', 'city-state'):\n",
    "        action_items.append({\n",
    "            'priority': 'high',\n",
    "            'type': 'content',\n",
    "            'query_or_url': row['Query'],\n",
    "            'action': f'Optimize content for this {row[\"Cluster\"]} query — position {row[\"Position\"]:.1f}, {row[\"Impressions\"]} impressions',\n",
    "            'impressions': row['Impressions'],\n",
    "            'expected_impact': 'Move from page 1-2 to top 3'\n",
    "        })\n",
    "\n",
    "# 3. Legacy URL redirects\n",
    "for _, row in legacy_urls.head(20).iterrows():\n",
    "    slug = urlparse(row['Page']).query.replace('clinics=', '')\n",
    "    action_items.append({\n",
    "        'priority': 'high' if row['Impressions'] >= 1000 else 'medium',\n",
    "        'type': 'redirect',\n",
    "        'query_or_url': row['Page'],\n",
    "        'action': f'Redirect legacy /?clinics={slug} to canonical URL — {row[\"Impressions\"]:,} impressions',\n",
    "        'impressions': row['Impressions'],\n",
    "        'expected_impact': 'Consolidate ranking signals to canonical URL'\n",
    "    })\n",
    "\n",
    "# 4. www redirect consolidation\n",
    "for _, row in www_pages.head(10).iterrows():\n",
    "    action_items.append({\n",
    "        'priority': 'medium',\n",
    "        'type': 'redirect',\n",
    "        'query_or_url': row['Page'],\n",
    "        'action': f'Ensure www → non-www redirect — {row[\"Impressions\"]:,} impressions on www version',\n",
    "        'impressions': row['Impressions'],\n",
    "        'expected_impact': 'Consolidate duplicate URLs'\n",
    "    })\n",
    "\n",
    "# 5. Treatment content gaps — high impression treatment queries with poor ranking\n",
    "treatment_gaps = treatment_queries[\n",
    "    (treatment_queries['Position'] > 15) &\n",
    "    (treatment_queries['Impressions'] >= 50)\n",
    "]\n",
    "for _, row in treatment_gaps.iterrows():\n",
    "    action_items.append({\n",
    "        'priority': 'medium',\n",
    "        'type': 'new-page',\n",
    "        'query_or_url': row['Query'],\n",
    "        'action': f'Consider treatment landing page — currently position {row[\"Position\"]:.0f} for {row[\"Impressions\"]} impressions',\n",
    "        'impressions': row['Impressions'],\n",
    "        'expected_impact': 'Dedicated content could rank for treatment queries'\n",
    "    })\n",
    "\n",
    "# 6. Underperforming pages need meta/content fixes\n",
    "for _, row in underperforming_pages.head(15).iterrows():\n",
    "    action_items.append({\n",
    "        'priority': 'high' if row['Impressions'] >= 2000 else 'medium',\n",
    "        'type': 'meta',\n",
    "        'query_or_url': row['Page'],\n",
    "        'action': f'Improve meta for underperforming page — {row[\"Impressions\"]:,} impressions, {row[\"CTR\"]:.1f}% CTR, position {row[\"Position\"]:.1f}',\n",
    "        'impressions': row['Impressions'],\n",
    "        'expected_impact': 'Higher CTR from existing impressions'\n",
    "    })\n",
    "\n",
    "actions_df = pd.DataFrame(action_items)\n",
    "# Sort: high priority first, then by impressions\n",
    "priority_order = {'high': 0, 'medium': 1, 'low': 2}\n",
    "actions_df['_priority_sort'] = actions_df['priority'].map(priority_order)\n",
    "actions_df = actions_df.sort_values(['_priority_sort', 'impressions'], ascending=[True, False]).drop(columns='_priority_sort')\n",
    "\n",
    "print(f'Total action items: {len(actions_df)}')\n",
    "print(f'  High priority: {(actions_df[\"priority\"] == \"high\").sum()}')\n",
    "print(f'  Medium priority: {(actions_df[\"priority\"] == \"medium\").sum()}')\n",
    "print()\n",
    "actions_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Export Output CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all output CSVs\n",
    "out = AUDIT_DIR\n",
    "\n",
    "# 1. Query clusters\n",
    "queries[['Query', 'Clicks', 'Impressions', 'CTR', 'Position', 'Cluster', 'PosBucket']].to_csv(\n",
    "    out / 'query-clusters.csv', index=False\n",
    ")\n",
    "\n",
    "# 2. Page performance\n",
    "pages[['Page', 'Clicks', 'Impressions', 'CTR', 'Position', 'PageType']].to_csv(\n",
    "    out / 'page-performance.csv', index=False\n",
    ")\n",
    "\n",
    "# 3. Quick wins\n",
    "quick_wins[['Query', 'Clicks', 'Impressions', 'CTR', 'Position', 'Cluster']].to_csv(\n",
    "    out / 'quick-wins.csv', index=False\n",
    ")\n",
    "\n",
    "# 4. Content gaps (high-value non-brand queries)\n",
    "high_value_nonbrand[['Query', 'Clicks', 'Impressions', 'CTR', 'Position', 'Cluster']].to_csv(\n",
    "    out / 'content-gaps.csv', index=False\n",
    ")\n",
    "\n",
    "# 5. Action items\n",
    "actions_df.to_csv(out / 'action-items.csv', index=False)\n",
    "\n",
    "# 6. Index gaps (legacy + www URLs as proxy since full coverage export is date-level only)\n",
    "index_gap_rows = []\n",
    "for _, row in legacy_urls.iterrows():\n",
    "    index_gap_rows.append({'url': row['Page'], 'issue': 'legacy-url-format', 'impressions': row['Impressions']})\n",
    "for _, row in www_pages.iterrows():\n",
    "    index_gap_rows.append({'url': row['Page'], 'issue': 'www-duplicate', 'impressions': row['Impressions']})\n",
    "\n",
    "index_gaps_df = pd.DataFrame(index_gap_rows).sort_values('impressions', ascending=False) if index_gap_rows else pd.DataFrame(columns=['url', 'issue', 'impressions'])\n",
    "index_gaps_df.to_csv(out / 'index-gaps.csv', index=False)\n",
    "\n",
    "print('Exported files:')\n",
    "for f in ['query-clusters.csv', 'page-performance.csv', 'quick-wins.csv', 'content-gaps.csv', 'action-items.csv', 'index-gaps.csv']:\n",
    "    path = out / f\n",
    "    if path.exists():\n",
    "        rows = len(pd.read_csv(path))\n",
    "        print(f'  {f}: {rows} rows')\n",
    "    else:\n",
    "        print(f'  {f}: NOT FOUND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Review the exported CSVs and the analysis above. Key things to look for:\n",
    "\n",
    "1. **Quick wins** — queries at position 4-15 where small content/meta improvements can move you to top 3\n",
    "2. **CTR anomalies** — good positions but poor click-through rates mean bad titles/descriptions\n",
    "3. **Near-me gaps** — high-impression generic queries where city/state pages need enrichment\n",
    "4. **Legacy URL consolidation** — `/?clinics=` URLs still getting impressions need redirects\n",
    "5. **Treatment content gaps** — treatment queries with no dedicated landing page\n",
    "\n",
    "Proceed to Step 3 (implementation) based on the action-items.csv priorities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}